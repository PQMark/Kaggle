{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import models.c_model\n",
    "import models.model\n",
    "\n",
    "import models.s_model\n",
    "import models.d_model\n",
    "import models.d2_model\n",
    "import models.n_model\n",
    "import scripts.trainer \n",
    "import scripts.train\n",
    "import scripts.eval\n",
    "import scripts.test\n",
    "import customs.focal_loss\n",
    "import scripts.data_loader\n",
    "importlib.reload(scripts.data_loader)\n",
    "importlib.reload(models.n_model)\n",
    "importlib.reload(models.d2_model)\n",
    "importlib.reload(customs.focal_loss)\n",
    "importlib.reload(scripts.test)\n",
    "importlib.reload(scripts.eval)\n",
    "importlib.reload(models.d_model)\n",
    "importlib.reload(scripts.train)\n",
    "importlib.reload(scripts.trainer)\n",
    "importlib.reload(models.c_model) \n",
    "importlib.reload(models.model) \n",
    "importlib.reload(models.try2) \n",
    "importlib.reload(models.s_model)\n",
    "\n",
    "from models.d_model import MergeNet\n",
    "from models.s_model import S_model\n",
    "from models.c_model import Network\n",
    "from models.model import MixerMLP, initialize_weights\n",
    "from models.try2 import MLPMixer\n",
    "from models.d2_model import MergeNet2\n",
    "from models.n_model import DenseNet\n",
    "\n",
    "from customs.focal_loss import FocalLoss\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "from scripts.train import train\n",
    "from scripts.eval import eval\n",
    "from scripts.test import test\n",
    "from scripts.trainer import Trainer\n",
    "from scripts.data_loader import AudioDatasetModule\n",
    "\n",
    "from torchsummaryX import summary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PHONEME LIST\n",
    "PHONEMES = [\n",
    "            '[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n",
    "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
    "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
    "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
    "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
    "            'V',     'W',     'Y',     'Z',     'ZH',    '[SOS]', '[EOS]']\n",
    "\n",
    "config = {\n",
    "    'subset': 1.0, # Subset of dataset to use (1.0 == 100% of data)\n",
    "    'context': 24,  # 30\n",
    "    'activations': 'Swish',\n",
    "    'learning_rate': 1e-3,\n",
    "    'dropout': 0.3,\n",
    "    'optimizers': 'AdamW',\n",
    "    'scheduler': 'OneCycleLR',\n",
    "    'epochs': 100,       # 30\n",
    "    'batch_size': 2048, # 1024, 500\n",
    "    'patience': 30,  \n",
    "    'save_every': 1,\n",
    "    'weight_decay': 0.01,\n",
    "    'weight_initialization': 'xavier_normal', # e.g kaiming_normal, kaiming_uniform, uniform, xavier_normal or xavier_uniform\n",
    "    'augmentations': 'FreqMask', # Options: [\"FreqMask\", \"TimeMask\", \"Both\", null]\n",
    "    'freq_mask_param': 4, #4\n",
    "    'time_mask_param': 8\n",
    " }\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "def clean_cache(device):\n",
    "    if device == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    elif device == \"cuda\":\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = AudioDatasetModule(\n",
    "    root=\"../autodl-fs/data\",\n",
    "    phonemes=PHONEMES,\n",
    "    train_partition=\"train-clean-100\",\n",
    "    val_partition=\"dev-clean\",\n",
    "    test_partition=\"test-clean\",\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    config=config,\n",
    "    num_workers=14,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "dm.initialize(mode=\"fit\")\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader   = dm.val_dataloader()\n",
    "\n",
    "dm.initialize(mode=\"test\")\n",
    "test_loader = dm.test_dataloader()\n",
    "\n",
    "print(\"Batch size     : \", config['batch_size'])\n",
    "print(\"Context        : \", config['context'])\n",
    "print(\"Input size     : \", (2*config['context']+1)*28)\n",
    "print(\"Output symbols : \", len(PHONEMES))\n",
    "\n",
    "print(\"batches = {}\".format(len(train_loader)))\n",
    "print(\"batches = {}\".format(len(val_loader)))\n",
    "print(\"batches = {}\".format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_n = DenseNet(arch=(4096, 2048, 1024, 1024, 750, 512), \n",
    "                   num_ouputs=round(len(PHONEMES)), \n",
    "                   dropout=(0.2, 0.15, 0.15, 0.15, 0.05, 0)).to(device)\n",
    "\n",
    "model = model_n\n",
    "model_name = \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "inputs, _ = next(iter(train_loader))\n",
    "model.apply_init(inputs.to(device), initialize_weights)\n",
    "\n",
    "print(inputs.shape)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# criterion = FocalLoss(gamma=1.5)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=len(train_loader), T_mult=1, \n",
    "#                                                                  eta_min = 0.0001)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=round(len(train_loader) * 1.2), \n",
    "#                                                                  eta_min = 0.00005)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3 * len(train_loader), gamma=0.9)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=2e-3, \n",
    "    total_steps = 20 * len(train_loader), \n",
    "    pct_start = 0.15, \n",
    "    anneal_strategy=\"cos\"\n",
    ")\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=True) \n",
    "\n",
    "clean_cache(device)\n",
    "gc.collect()\n",
    "\n",
    "start=0\n",
    "best_val_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb login \n",
    "wandb.login(key=\"......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test\n",
    "wandb.unwatch(model)\n",
    "test_Trainer = Trainer(config[\"epochs\"], criterion, optimizer, scheduler,\n",
    "                       config[\"patience\"], config[\"save_every\"], model_name, device=device, scaler=scaler)\n",
    "test_Trainer.fit(model, train_loader, val_loader, log=False, save_best=False,\n",
    "                 checkpoints=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wandb run\n",
    "run = wandb.init(\n",
    "    name    = f\"{model_name}_run_1\", ### set run names\n",
    "    reinit  = True, ### Allows reinitalizing runs when re-running this cell\n",
    "    #id     = \"\", ### Insert specific run id here if resuming a previous run\n",
    "    #resume = \"must\", ### need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"HW1P2\", ### Project name\n",
    "    group=f\"{model_name}\", \n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cache(device)\n",
    "gc.collect()\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "trainer = Trainer(config[\"epochs\"], criterion, optimizer, scheduler, \n",
    "                       config[\"patience\"], config[\"save_every\"], model_name, \n",
    "                       start=start, best_val_acc=best_val_acc, log_freq=20, \n",
    "                       device=device, scaler=scaler)\n",
    "trainer.fit(model, train_loader, val_loader, save_best=True, checkpoints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"----------------------------------------------------------------------\"\n",
    "\"----------------------------Resume Run--------------------------------\"\n",
    "\"----------------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint = torch.load(\"checkpoints/model/model_epoch_40.pth\")\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "# scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "start=40\n",
    "best_val_acc = 0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    name    = f\"{model_name}_run_1\",\n",
    "    reinit  = True,\n",
    "    id      = '......',   ### ID for the run\n",
    "    resume  = \"must\", \n",
    "    project = \"HW1P2\",\n",
    "    group=f\"{model_name}\",\n",
    "    config=config\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
